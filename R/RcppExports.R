# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Rcpp Fit Gaussian Process Classification using MCMC
#'
#' Pseudo marginal approach to fitting a Gaussian process classification model using Markov Chain Monte Carlo (MCMC)
#'
#' @param y binary output vector in {-1, +1}
#' @param X predictor matrix
#' @param nsteps number of iterations to run the MCMC sampler for
#' @param nburn number of 'burn in'
#' @param nimp number of samples in importance sampling to approximate the marginal likelihood
#' @param init_theta initial conditions for hyperparameter vector
#' @param init_marginal_lik initial condition for marginal likelihood
#' @param init_f initial condition for latent variables f
#' @param prior_density R function returning the prior density at a single point
#' @param kernel R function taking two inputs to calculate the kernel matrix
#' @param prop_dist_sample R function giving a sample from the proposal distribution
#' @param prop_dist_density R function giving the density from the proposal distribution
#' @param print_every number of steps until progress is printed to the console
#' @param debug logical; if \code{TRUE} then debugging information is printed to the console at each step
#' @param chain_no chain that is currently running, only used for printing
#' @param kernel_pass string; contains information as to whether the \code{kernel} argument is a function or a string. If a string detailing the kernel to use, a parallel construction of the gram matrix is implemented
#'
#' @return a single chain consisting of samples of the hyperparameter vector and samples of the latent variable \eqn{f}
#'
#' @details
#' Combination of \code{\link{laplace_approx}}, \code{\link{get_approx_marginal}} and \code{\link{ell_ss_sample}}, coming
#' together to fit a Gaussian process classification model using MCMC.
#'
#' This function retrieves output from all of the above functions, calculates the acceptance probability and accepts or rejects a sample.
#' The accepted samples are saved in a matrix.
#'
algo_1 <- function(y, X, nsteps, nburn, nimp, init_theta, init_marginal_lik, init_f, prior_density, kernel, prop_dist_sample, prop_dist_density, print_every, debug, chain_no, kernel_pass) {
    .Call(`_gpc_algo_1`, y, X, nsteps, nburn, nimp, init_theta, init_marginal_lik, init_f, prior_density, kernel, prop_dist_sample, prop_dist_density, print_every, debug, chain_no, kernel_pass)
}

#' Get Pseudo Marginal Likelihood
#'
#' Unbiased estimation of \eqn{p(y | \theta)} using importance sampling.
#'
#' @param y binary output vector in -1, +1
#' @param K Gram matrix
#' @param nimp number of samples in importance sampling to approximate the marginal likelihood
#' @param theta hyperparameter vector
#' @param laplace_approx list containing \code{f_hat} and \code{sigma_hat}, output from \code{\link{laplace_approx}}
#'
#' @return a single value, the log sum of the pseudo weights.
#'
#' @details
#' Using the approximating distribution \eqn{q(f | y, \theta)}, the unbiased estimate of the marginal can be given as
#' \deqn{
#' \tilde{p} (y | \theta) \approx 1 / N_{imp} \sum^{N_{imp}}_{i=1} p(y | f_i) p(f_i | \theta) / q(f | y, \theta)
#' }
#' In this case, the approximating distribution \eqn{q(f | y, \theta)} is given by the laplace approximation,
#' which is calculated in \code{\link{laplace_approx}}, \eqn{p(y | f)} is the likelihood and p(f | \theta) is the prior density.
#'
get_approx_marginal <- function(y, K, nimp, theta, laplace_approx) {
    .Call(`_gpc_get_approx_marginal`, y, K, nimp, theta, laplace_approx)
}

get_approx_marginal_par <- function(y, K, nimp, theta, laplace_approx) {
    .Call(`_gpc_get_approx_marginal_par`, y, K, nimp, theta, laplace_approx)
}

#' Elliptical Slice Sampling
#'
#' Elliptical method to sample latent variables \code{f}
#'
#' @param y binary output vector in -1, +1
#' @param f latent variable for Gaussian process
#' @param K Gram matrix
#'
#' @return new proposal for \eqn{f}
#'
#' @details
#' Sampling of latent variables \code{f} constrained to an ellipse centred at the current mean
#' \eqn{f}, with eccentricity of the ellipse determined by the covariance matrix \eqn{\Sigma}.
#'
ell_ss_sample <- function(y, f, K) {
    .Call(`_gpc_ell_ss_sample`, y, f, K)
}

build_K <- function(x, y, k, theta) {
    .Call(`_gpc_build_K`, x, y, k, theta)
}

make_gram_par <- function(x, y, theta) {
    .Call(`_gpc_make_gram_par`, x, y, theta)
}

#' Laplace Approximation
#'
#' Laplace approximation of the posterior of the latent Variables \eqn{f}.
#'
#' @param y binary output vector in -1, +1
#' @param K Gram matrix
#'
#' @return list containing two elements:
#' \item{\code{f_hat}}{mean of the approximate Gaussian distribution}
#' \item{\code{sigma_hat}}{covariance matrix of the approximate Gaussian distribution}
#'
#' @details
#' Obtain a Laplace approximate of the posterior of \deqn{
#' p(f | y, \theta)
#' }
#' by approximating
#' the distribution with a Gaussian \eqn{N(f | \mu_q, \Sigma_q)}.
#'
#' This amounts to an iterative procedure, iterating a Newton-Raphson formula:
#' \deqn{
#' f_{new} = f - (\nabla_f \nabla_f \Psi(f))^{-1} \nabla_f \Psi(f),
#' }
#' where \deqn{
#' \Psi(f) = log p(y | f) + log p (f | \theta) + const.
#' }
#' Beginning from \eqn{f=0}, this
#' is iterated until convergence. That is, when the distance between \eqn{f} and \eqn{f_{new}} becomes
#' sufficiently small.
laplace_approx <- function(y, K) {
    .Call(`_gpc_laplace_approx`, y, K)
}

#' @keywords internal
log_lik <- function(y, f) {
    .Call(`_gpc_log_lik`, y, f)
}

#' @keywords internal
d_log_lik <- function(y, f) {
    .Call(`_gpc_d_log_lik`, y, f)
}

#' @keywords internal
d2_log_lik <- function(y, f) {
    .Call(`_gpc_d2_log_lik`, y, f)
}

#' @keywords internal
rmvnorm_cpp <- function(n, mu, sigma) {
    .Call(`_gpc_rmvnorm`, n, mu, sigma)
}

#' @keywords internal
dmvnorm_cpp <- function(x, mean, sigma, logd) {
    .Call(`_gpc_dmvnorm`, x, mean, sigma, logd)
}

#' Rcpp Predict Gaussian Process Classification using MCMC
#'
#' Pseudo marginal approach to fitting a Gaussian process classification model using Markov Chain Monte Carlo (MCMC)
#'
#' @param y binary output vector in {-1, +1}
#' @param X predictor matrix
#' @param newdata new data to predict from, same dimension as \code{X}
#' @param kernel R function taking two inputs to calculate the kernel matrix
#' @param fit list containing samples from the mcmc chains for \code{f} and \code{theta}
#' @param print_every number of steps until progress is printed to the console
#'
#' @return fitted probabilities corresponding to the positive and negative class for each data point in \code{newdata}
#'
predict_gp <- function(y, X, newdata, kernel, fit, print_every) {
    .Call(`_gpc_predict_gp`, y, X, newdata, kernel, fit, print_every)
}

#' @keywords internal
mat_to_rcpp <- function(X) {
    .Call(`_gpc_mat_to_rcpp`, X)
}

#' @keywords internal
rcpp_to_mat <- function(X) {
    .Call(`_gpc_rcpp_to_mat`, X)
}

#' @keywords internal
vec_to_rcpp <- function(x) {
    .Call(`_gpc_vec_to_rcpp`, x)
}

#' @keywords internal
rcpp_to_vec <- function(x) {
    .Call(`_gpc_rcpp_to_vec`, x)
}

#' @keywords internal
NULL

#' @keywords internal
NULL

#' @keywords internal
NULL

#' @keywords internal
NULL

#' @keywords internal
NULL

